# Mini-SOC-Phase3

In this Phase I want to implement EDR solution, send alerts to Sentinel, integrate Threat Intelligence and install EDR agent on endpoint and simulate some attacks.

Let's start with EDR. I've choosed Wazuh EDR, as it's free and open-source. Head on to official website and download.
Unfortunetely Debian is not supported by Wazuh and installation did not succeed (mention that ignore hardware requirements flag was used during installtion). So, hoping that this is not due to hardware limitations, I decided to change this VM to Ubuntu.

Go to Sentinel - Data Connectors and detach this VM. 
Optionaly: wait until azure monitor agent checks in, fetches new DCR and stop heartbeating. Not heartbeats for 2 minutes - considering agent has stopped gracefully.

Let's remove this VM and deploy a new one with Ubuntu. Try to log in - Voila new Ubuntu machine. But ... still Wazuh does not want to install. I checked installation logs and it appeared that JVM could not start due to low memory. I recalled that I did not change VM size and it had just 1 gig, lol. So, I resized it to Standard DS1 v2 (1 vcpu, 3.5 GiB memory)

Wazuh installed succesfully! 
(screen)
Before trying to access GUI, let's create FW policy
(screen)
Web UI is accessible, use credentials from installation log to log in
(screen)

Also, let's recover logging capabilities of this machine, by install rsyslog and configuring it as we did in Phase 2.
Quick installation and configuration and we can see Palo logs arriving.
(screenshot)

Go to Wazuh dashboard and click "Deploy Agent", select Deb Amd64 arch package, fill in management server IP, copy link and paste on target host. After installation also run commands to reload daemon list, enable auto startup and start agent.
(screen)

Let's monitor traffic and see if which ports are used by Wazuh Agent. As we can see agent is communicating over port 1515 and 1514
(screen)

Let's create a policy to allow it. Commit changes.
(screen)

Go to Wazuh Dashboard - Agent Management - Summary. Here we can see our agent checking-in!
(screen)

Dashboard and some test logs.
(screen)

Let me explain the concept here. The initial idea was to create a fully integrated Mini Security Operation Centre with a little bit of automation. Traffic to Honeypot should have transited via FW and inspected with FW AV, IPS, URL-Filter, Anti-Spyware features. Generated alerts should've been forwarded to Sentinel, create incident, correlate with logs from Honeypot and trigger automation to block threat.
However 1) issue with routing did not allow Honeypot to see real attackers ip 2) VM did not have a serial number and I was not able to recover it with various methods, which does not allow to register firewall and get a license, hence I'm not able to use those Threat Prevention features (av, ips, etc.) So, in this phase we will only create automation for alerts from Wazuh.

Sending all logs from Wazuh to Sentinel is unnecessary, so I will forward only alerts.

First, open /var/etc/osseec.conf and add following lines to enable forwarding of alerts to local rsyslog in CEF format.
(screen)

systemctl restart wazuh-manager

Let's test if EDR agent is working and if any alerts make their way to syslog. I'm running Kali Linux on WSL. I installed "Hydra" - app for bruteforcing, downloaded a common wordlist "rockyou.txt" and started bruteforcing SSH port of our target machine.
(screen)

We can see alerts generated by our attack. 
(screen)
Also in syslog:
(screen)

The DCR we created to collect logs from Palo Alto is also collecting alerts from Wazuh as they are in CEF format and DCR is set up to collect everything. So, we should be able to see these alerts in Sentinel.
(screen)

Only receiving alerts is not enough, we should utilize more capabilities of Sentinel, so let's create Analytics Rule, that will query for these alerts, and create incident.
Click on "New Alert Rule", fill in the name, severy. 
(screen)
On next tab we can specify query, which will be run and timeframe, how often will it be run and how old should be logs that it would be checking. Set alert treshold to > 0 and Group all events into single alerts to not make to much noise.
Click on "Entity mapping" and add "IP address" and select appropriate field from logs. This "Entity" will later be passed to playbook.
(screen)
In the next tab - check "Create Incidents" and "Group Alerts".
(screen)

Now that our rule is created, let's test it by running another bruteforce attack.
(screen)
As we can see, after some time incident is automatically created
(screen)

Sentinel allows us to investigate incident, see timeline of events, entities, run playbook and automatically correlate logs.
(screen)

Let's proceed with playbook creation. First, we need to get our FW' API key, so we can integrate it with Sentinel automation. Automation will work in the following way: there's would be a policy on FW with Blacklist address group in destination and another policy with this group in source. When automation rule is triggered in Sentinel - it connects to Palo Alto via API and update Blacklist address group with new IP, which was taken from analytics rules (entity mapping). 

Create following address group and policies
(screen) (screen)

Let's get API key - issue following command, replace fields with IP of your FW, login and password.
(screen)
(screen)

There is a connector template on Azure's github
https://github.com/Azure/Azure-Sentinel/tree/master/Playbooks/PaloAlto-PAN-OS
Click "Deploy to Azure", fill int Firewall's IP and deploy. Remember to use FW's public IP, as connection will be initilized from Azure public IP range. I initialy made error by providing FW's private mgmt IP and had to re-deploy the connector.
(screen)

At this moment I ran out of free credits and had to set up a new account on Azure. I recreated the same network, but without VPN (because it appeared to be the most cost eating service) and without all policies, because I have demonstrated it in previous phase.
(screen)

Then, go to Content Hub and install "PaloAlto-PAN-OS-BlockIP" playbook. After installing click "Create playbook". Fill in the Address Group name and some dummy string in Teams Group and channel ID. Then, go to newly deployed "Paloaltoconnector-PaloAlto-PAN-OS-BlockIP" API Connection (you can find in Resource Group) and fill in the API key.
(screen)
On a test run I encountered following error and came to conclusion that Azure does not want to establish connection because FW has self signed certificate. And I was correct. https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app?tabs=azure-portal#access-for-outbound-calls-to-other-services-and-systems
(screen)

Since the connector is "Consumption logic app", firewall needs to have a trusted certificate. To receive a trusted certificate, we need to request it from a globally trusted CA and we need to have a domain to prove ownership. So, I bought a cheap domain at Namecheap, requested certificate from ZeroSSL, proved that I own this domain by inserting a requested CNAME record.
(screen)
Add "A" record to point to FW's public IP.

Now, we need to replace firewalls certificate with newly generated, trusted one. Go to Device - Certificate and import new certificate. Then go to SSl/TLS Service Profile and create new profile to leverage our new certificate. Final step is set up firewall to use new profile - Go to Management - General Settings, click on gear icon and select newly create profile. Commit change. Web service will restart
(screen)

And Voila! We are now able to access our FW by domain name.
(screen)

On a test run of automation we can see, that SSl/TLS error dissapeared. But ... a new one was encountered. 
(screen)

This connector is leveraging Microsoft Teams API to post a message to channel and allow user to either block IP or pass. Moreover, this cannot be skipped, as one logic block is configred to wait for user's response on Teams. And execution is hold until response is received.
(screen)

I started googling how to get Teams API key, found a free personal plan for Teams, but this plan does not allow creation of "Teams" and channels, so I'm not able to get IDs for connector to work. (At that point of time I did not know there was a free trial for business plan :D).

So, I started messing with the app's logic. After analysis, many trials and strange errors I was finally able to exclude Team's logic. Next screenshot shows stripped version of app.
(screen)

Let's trigger alert and see what happens! Incident was triggered - everything works, we can see the Address Group on FW updated with new IP.
(screen)

But real public IP is hidden behind NAT. So I decided to create (with the help of AI) a correlation query to get a pre-NAT IP from firewall logs.
(screen)

Let's put this query into our rule and test everything.
(screen)

Works like a charm!
(screen)

Conclusion.
